# æ–‡æœ¬å¤„ç†å·¥å…·é›† (Text Processing Tools)

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„æ–‡æœ¬å¤„ç†å·¥å…·é›†ï¼Œæ”¯æŒå¥å­åˆ†å‰²ã€tokenåˆ†å‰²ã€éš¾åº¦è¯„ä¼°ã€lemmaå¤„ç†ã€vocabç”Ÿæˆç­‰åŠŸèƒ½ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
preprocessing/
â”œâ”€â”€ src/                          # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_processor.py     # ä¸»æ–‡æœ¬å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ token_data.py         # æ•°æ®ç»“æ„å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ sentence_splitter.py  # å¥å­åˆ†å‰²å™¨
â”‚   â”‚   â””â”€â”€ token_splitter.py     # Tokenåˆ†å‰²å™¨
â”‚   â”œâ”€â”€ agents/                   # AIä»£ç†æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ get_difficulty_agent.py           # éš¾åº¦è¯„ä¼°ä»£ç†
â”‚   â”‚   â”œâ”€â”€ single_token_difficulty_estimation.py  # å•tokenéš¾åº¦è¯„ä¼°
â”‚   â”‚   â”œâ”€â”€ sub_assistant.py      # å­åŠ©æ‰‹
â”‚   â”‚   â”œâ”€â”€ vocab_explanation.py  # è¯æ±‡è§£é‡ŠåŠ©æ‰‹
â”‚   â”‚   â”œâ”€â”€ vocab_example_explanation.py  # è¯æ±‡ç¤ºä¾‹è§£é‡ŠåŠ©æ‰‹
â”‚   â”‚   â””â”€â”€ grammar_analysis.py   # è¯­æ³•åˆ†æåŠ©æ‰‹
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ get_lemma.py          # Lemmaå¤„ç†
â”‚   â”‚   â”œâ”€â”€ get_pos_tag.py        # POSæ ‡ç­¾å¤„ç†
â”‚   â”‚   â”œâ”€â”€ openai_utils.py       # OpenAIå·¥å…·
â”‚   â”‚   â”œâ”€â”€ token_to_vocab.py     # Tokenåˆ°Vocabè½¬æ¢
â”‚   â”‚   â”œâ”€â”€ utility.py            # é€šç”¨å·¥å…·
â”‚   â”‚   â”œâ”€â”€ config.py             # é…ç½®
â”‚   â”‚   â””â”€â”€ promp.py              # æç¤ºæ¨¡æ¿
â”‚   â””â”€â”€ tests/                    # æµ‹è¯•æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_difficulty_integration.py    # éš¾åº¦è¯„ä¼°é›†æˆæµ‹è¯•
â”‚       â”œâ”€â”€ test_lemma_integration.py         # Lemmaé›†æˆæµ‹è¯•
â”‚       â”œâ”€â”€ test_difficulty_estimation.py     # éš¾åº¦è¯„ä¼°æµ‹è¯•
â”‚       â”œâ”€â”€ test_token_to_vocab.py            # Tokenåˆ°Vocabè½¬æ¢æµ‹è¯•
â”‚       â””â”€â”€ verify_processing.py  # å¤„ç†éªŒè¯
â”œâ”€â”€ examples/                     # ç¤ºä¾‹å’Œæ¼”ç¤º
â”‚   â”œâ”€â”€ example_openai_usage.py   # OpenAIä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ vocab_generation_example.py  # Vocabç”Ÿæˆç¤ºä¾‹
â”‚   â”œâ”€â”€ grammar_analysis_example.py  # è¯­æ³•åˆ†æç¤ºä¾‹
â”‚   â”œâ”€â”€ final_demo.py             # æœ€ç»ˆæ¼”ç¤º
â”‚   â””â”€â”€ test_article_5sentences.txt  # æµ‹è¯•æ–‡æœ¬
â”œâ”€â”€ docs/                         # æ–‡æ¡£
â”‚   â”œâ”€â”€ README_TextProcessor.md   # æ–‡æœ¬å¤„ç†å™¨æ–‡æ¡£
â”‚   â”œâ”€â”€ README_OpenAI.md          # OpenAIä½¿ç”¨æ–‡æ¡£
â”‚   â””â”€â”€ README_Difficulty_Integration.md  # éš¾åº¦è¯„ä¼°é›†æˆæ–‡æ¡£
â”œâ”€â”€ data/                         # æ•°æ®è¾“å‡ºç›®å½•
â”œâ”€â”€ main.py                       # ä¸»å…¥å£æ–‡ä»¶
â””â”€â”€ README.md                     # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. åŸºæœ¬ä½¿ç”¨

```bash
# å¤„ç†å•ä¸ªæ–‡ä»¶
python main.py examples/test_article_5sentences.txt 1

# å¤„ç†å¤šä¸ªæ–‡ä»¶
python -c "
from src.core.text_processor import TextProcessor
processor = TextProcessor()
processor.process_multiple_files(['file1.txt', 'file2.txt'], start_text_id=1)
"
```

### 3. ç”ŸæˆVocab

```bash
# ä»æ–‡æœ¬ç”Ÿæˆvocab
python examples/vocab_generation_example.py

# ä½¿ç”¨APIç”Ÿæˆvocab
python -c "
from src.core.text_processor import TextProcessor
from src.utils.token_to_vocab import TokenToVocabConverter

# å¤„ç†æ–‡æœ¬
processor = TextProcessor()
text = 'Artificial intelligence has revolutionized technology.'
original_text = processor.process_text_to_structured_data(text, 1, 'æµ‹è¯•æ–‡æœ¬')

# ç”Ÿæˆvocab
converter = TokenToVocabConverter()
vocab_expressions = converter.convert_tokens_from_text(original_text)
converter.save_vocab_data(vocab_expressions)
"
```

### 4. è¯­æ³•åˆ†æ

```bash
# è¿è¡Œè¯­æ³•åˆ†æç¤ºä¾‹
python examples/grammar_analysis_example.py

# ä½¿ç”¨APIè¿›è¡Œè¯­æ³•åˆ†æ
python -c "
from src.agents import GrammarAnalysisAssistant

# åˆ›å»ºè¯­æ³•åˆ†æåŠ©æ‰‹
assistant = GrammarAnalysisAssistant()

# åˆ†æå¥å­è¯­æ³•
result = assistant.analyze_grammar(
    'Artificial intelligence has revolutionized technology.',
    'This sentence discusses the impact of AI on technology.'
)

print(f'è¯­æ³•è®²è§£: {result[\"explanation\"]}')
print(f'å…³é”®è¯: {result[\"keywords\"]}')
"
```

### 4. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œéš¾åº¦è¯„ä¼°é›†æˆæµ‹è¯•
python src/tests/test_difficulty_integration.py

# è¿è¡Œlemmaé›†æˆæµ‹è¯•
python src/tests/test_lemma_integration.py

# è¿è¡Œtoken_to_vocabæµ‹è¯•
python src/tests/test_token_to_vocab.py

# è¿è¡Œå¤„ç†éªŒè¯
python src/tests/verify_processing.py
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. æ–‡æœ¬å¤„ç†å™¨ (TextProcessor)

- **å¥å­åˆ†å‰²**ï¼šæ™ºèƒ½åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­
- **Tokenåˆ†å‰²**ï¼šå°†å¥å­åˆ†å‰²ä¸ºtokensï¼ˆæ–‡æœ¬ã€æ ‡ç‚¹ã€ç©ºæ ¼ï¼‰
- **éš¾åº¦è¯„ä¼°**ï¼šå¯¹textç±»å‹tokenè¿›è¡Œéš¾åº¦è¯„ä¼°
- **Lemmaå¤„ç†**ï¼šå¯¹textç±»å‹tokenè¿›è¡Œlemmaå¤„ç†
- **ç»“æ„åŒ–è¾“å‡º**ï¼šç”ŸæˆJSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®

### 2. AIä»£ç†åŠŸèƒ½

- **éš¾åº¦è¯„ä¼°**ï¼šä½¿ç”¨OpenAI APIè¯„ä¼°tokenéš¾åº¦
- **è¯æ±‡è§£é‡Š**ï¼šç”Ÿæˆè¯¦ç»†çš„è¯æ±‡è§£é‡Š
- **ä¸Šä¸‹æ–‡è§£é‡Š**ï¼šç”Ÿæˆè¯æ±‡åœ¨ç‰¹å®šè¯­å¢ƒä¸­çš„è§£é‡Š
- **è¯­æ³•åˆ†æ**ï¼šåˆ†æå¥å­çš„è¯­æ³•ç»“æ„å’Œå…³é”®è¯
- **æ™ºèƒ½åˆ†æ**ï¼šæ”¯æŒä¸Šä¸‹æ–‡ç›¸å…³çš„åˆ†æ
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶

### 3. Tokenåˆ°Vocabè½¬æ¢

- **è‡ªåŠ¨è¯†åˆ«**ï¼šè‡ªåŠ¨è¯†åˆ«hardéš¾åº¦çš„textç±»å‹token
- **è¯æ±‡è§£é‡Š**ï¼šä½¿ç”¨AIç”Ÿæˆè¯¦ç»†çš„è¯æ±‡è§£é‡Š
- **ä¸Šä¸‹æ–‡åˆ†æ**ï¼šåˆ†æè¯æ±‡åœ¨å¥å­ä¸­çš„å…·ä½“ç”¨æ³•
- **æ•°æ®æŒä¹…åŒ–**ï¼šå°†vocabæ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼
- **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªtoken

### 4. å·¥å…·æ¨¡å—

- **Lemmaå¤„ç†**ï¼šä½¿ç”¨NLTKè¿›è¡Œè¯å½¢è¿˜åŸ
- **POSæ ‡ç­¾**ï¼šè¯æ€§æ ‡æ³¨åŠŸèƒ½
- **OpenAIé›†æˆ**ï¼šOpenAI APIå·¥å…·ç±»

## ğŸ“Š è¾“å‡ºæ ¼å¼

### å¤„ç†åçš„æ–‡æœ¬æ•°æ®

```json
{
  "text_id": 1,
  "text_title": "ç¤ºä¾‹æ–‡æœ¬",
  "text_by_sentence": [
    {
      "sentence_id": 1,
      "sentence_body": "ç¤ºä¾‹å¥å­ã€‚",
      "tokens": [
        {
          "token_body": "ç¤ºä¾‹",
          "token_type": "text",
          "difficulty_level": "easy",
          "lemma": "ç¤ºä¾‹"
        }
      ]
    }
  ]
}
```

### ç”Ÿæˆçš„Vocabæ•°æ®

```json
{
  "vocab_expressions": [
    {
      "vocab_id": 1,
      "vocab_body": "Artificial",
      "explanation": "Artificial æ˜¯ä¸€ä¸ªå½¢å®¹è¯ï¼Œæ„æ€æ˜¯'äººé€ çš„'æˆ–'éè‡ªç„¶çš„'...",
      "source": "auto",
      "is_starred": false,
      "examples": [
        {
          "vocab_id": 1,
          "text_id": 1,
          "sentence_id": 1,
          "context_explanation": "åœ¨è¿™é‡Œï¼Œ'Artificial' æŒ‡çš„æ˜¯ç”±äººç±»åˆ›é€ æˆ–åˆ¶é€ çš„..."
        }
      ]
    }
  ],
  "next_vocab_id": 2
}
```

## ğŸ§ª æµ‹è¯•

é¡¹ç›®åŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

- **é›†æˆæµ‹è¯•**ï¼šæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½çš„é›†æˆ
- **å•å…ƒæµ‹è¯•**ï¼šæµ‹è¯•å„ä¸ªæ¨¡å—çš„åŠŸèƒ½
- **éªŒè¯æµ‹è¯•**ï¼šéªŒè¯æ•°æ®å¤„ç†çš„æ­£ç¡®æ€§
- **Vocabç”Ÿæˆæµ‹è¯•**ï¼šæµ‹è¯•tokenåˆ°vocabçš„è½¬æ¢åŠŸèƒ½

## ğŸ“š æ–‡æ¡£

è¯¦ç»†æ–‡æ¡£è¯·æŸ¥çœ‹ `docs/` ç›®å½•ï¼š

- [æ–‡æœ¬å¤„ç†å™¨æ–‡æ¡£](docs/README_TextProcessor.md)
- [OpenAIä½¿ç”¨æ–‡æ¡£](docs/README_OpenAI.md)
- [éš¾åº¦è¯„ä¼°é›†æˆæ–‡æ¡£](docs/README_Difficulty_Integration.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ï¿½ï¿½ è®¸å¯è¯

MIT License 