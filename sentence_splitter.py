import re
import json
import os
from typing import List, Union
from token_data import OriginalText, Sentence, Token
from token_splitter import split_tokens

def split_sentences(text: str) -> List[str]:
    """
    å°†æ–‡æœ¬æŒ‰å¥å­åˆ†éš”
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¥å·ã€é—®å·ã€æ„Ÿå¹å·ä½œä¸ºå¥å­ç»“æŸæ ‡è®°
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
    # åŒ¹é…å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼Œåé¢è·Ÿç€ç©ºæ ¼æˆ–æ¢è¡Œç¬¦
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºç™½
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    
    return sentences

def read_and_split_sentences(file_path: str) -> List[str]:
    """
    è¯»å–txtæ–‡ä»¶å¹¶è¿”å›åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        sentences = split_sentences(content)
        return sentences
    
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{file_path}'")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return []

def process_text_to_structured_data(text: Union[str, str], text_id: int, text_title: str = "") -> OriginalText:
    """
    å°†æ–‡æœ¬å¤„ç†æˆç»“æ„åŒ–æ•°æ®
    
    Args:
        text: æ–‡æœ¬å†…å®¹æˆ–æ–‡ä»¶è·¯å¾„
        text_id: æ–‡æœ¬ID
        text_title: æ–‡æœ¬æ ‡é¢˜
        
    Returns:
        OriginalText: ç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®
    """
    # å¦‚æœè¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå…ˆè¯»å–æ–‡ä»¶
    if os.path.isfile(text):
        with open(text, 'r', encoding='utf-8') as file:
            text_content = file.read()
        if not text_title:
            text_title = os.path.basename(text)
    else:
        text_content = text
        if not text_title:
            text_title = f"Text_{text_id}"
    
    # åˆ†å‰²å¥å­
    sentence_texts = split_sentences(text_content)
    
    # åˆ›å»ºå¥å­å¯¹è±¡åˆ—è¡¨
    sentences = []
    global_token_id = 0
    
    for sentence_id, sentence_text in enumerate(sentence_texts, 1):
        # åˆ†å‰²tokens
        token_dicts = split_tokens(sentence_text)
        
        # åˆ›å»ºTokenå¯¹è±¡åˆ—è¡¨
        tokens = []
        for token_id, token_dict in enumerate(token_dicts, 1):
            token = Token(
                token_body=token_dict["token_body"],
                token_type=token_dict["token_type"],
                global_token_id=global_token_id,
                sentence_token_id=token_id
            )
            tokens.append(token)
            global_token_id += 1
        
        # åˆ›å»ºSentenceå¯¹è±¡
        sentence = Sentence(
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_body=sentence_text,
            grammar_annotations=[],
            vocab_annotations=[],
            tokens=tokens
        )
        sentences.append(sentence)
    
    # åˆ›å»ºOriginalTextå¯¹è±¡
    original_text = OriginalText(
        text_id=text_id,
        text_title=text_title,
        text_by_sentence=sentences
    )
    
    return original_text

def save_structured_data(original_text: OriginalText, output_dir: str):
    """
    ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æŒ‡å®šç›®å½•
    
    Args:
        original_text: ç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜original_text.json
    original_text_data = {
        "text_id": original_text.text_id,
        "text_title": original_text.text_title,
        "text_by_sentence": [
            {
                "text_id": sentence.text_id,
                "sentence_id": sentence.sentence_id,
                "sentence_body": sentence.sentence_body,
                "grammar_annotations": sentence.grammar_annotations,
                "vocab_annotations": sentence.vocab_annotations,
                "tokens": [
                    {
                        "token_body": token.token_body,
                        "token_type": token.token_type,
                        "difficulty_level": token.difficulty_level,
                        "global_token_id": token.global_token_id,
                        "sentence_token_id": token.sentence_token_id,
                        "explanation": token.explanation,
                        "pos_tag": token.pos_tag,
                        "lemma": token.lemma,
                        "is_grammar_marker": token.is_grammar_marker
                    }
                    for token in sentence.tokens
                ]
            }
            for sentence in original_text.text_by_sentence
        ]
    }
    
    with open(os.path.join(output_dir, "original_text.json"), 'w', encoding='utf-8') as f:
        json.dump(original_text_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜sentences.json
    sentences_data = [
        {
            "text_id": sentence.text_id,
            "sentence_id": sentence.sentence_id,
            "sentence_body": sentence.sentence_body,
            "grammar_annotations": sentence.grammar_annotations,
            "vocab_annotations": sentence.vocab_annotations,
            "tokens": [
                {
                    "token_body": token.token_body,
                    "token_type": token.token_type,
                    "difficulty_level": token.difficulty_level,
                    "global_token_id": token.global_token_id,
                    "sentence_token_id": token.sentence_token_id,
                    "explanation": token.explanation,
                    "pos_tag": token.pos_tag,
                    "lemma": token.lemma,
                    "is_grammar_marker": token.is_grammar_marker
                }
                for token in sentence.tokens
            ]
        }
        for sentence in original_text.text_by_sentence
    ]
    
    with open(os.path.join(output_dir, "sentences.json"), 'w', encoding='utf-8') as f:
        json.dump(sentences_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜tokens.json (æ‰€æœ‰tokensçš„æ‰å¹³åŒ–åˆ—è¡¨)
    all_tokens = []
    for sentence in original_text.text_by_sentence:
        for token in sentence.tokens:
            all_tokens.append({
                "token_body": token.token_body,
                "token_type": token.token_type,
                "difficulty_level": token.difficulty_level,
                "global_token_id": token.global_token_id,
                "sentence_token_id": token.sentence_token_id,
                "sentence_id": sentence.sentence_id,
                "text_id": sentence.text_id,
                "explanation": token.explanation,
                "pos_tag": token.pos_tag,
                "lemma": token.lemma,
                "is_grammar_marker": token.is_grammar_marker
            })
    
    with open(os.path.join(output_dir, "tokens.json"), 'w', encoding='utf-8') as f:
        json.dump(all_tokens, f, ensure_ascii=False, indent=2)

def process_text_file(input_path: str, text_id: int, output_dir: str = None):
    """
    å¤„ç†æ–‡æœ¬æ–‡ä»¶å¹¶ä¿å­˜ç»“æ„åŒ–æ•°æ®
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        text_id: æ–‡æœ¬ID
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    if output_dir is None:
        output_dir = f"data/text_{text_id:03d}"
    
    # å¤„ç†æ–‡æœ¬
    original_text = process_text_to_structured_data(input_path, text_id)
    
    # ä¿å­˜æ•°æ®
    save_structured_data(original_text, output_dir)
    
    print(f"âœ… æ–‡æœ¬å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ å¥å­æ•°é‡: {len(original_text.text_by_sentence)}")
    print(f"ğŸ”¤ æ€»tokenæ•°é‡: {sum(len(sentence.tokens) for sentence in original_text.text_by_sentence)}")

def main():
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = "test_text.txt"
    
    print("æ­£åœ¨è¯»å–æ–‡ä»¶å¹¶åˆ†å‰²å¥å­...")
    sentences = read_and_split_sentences(test_file)
    
    if sentences:
        print(f"\nå…±æ‰¾åˆ° {len(sentences)} ä¸ªå¥å­ï¼š\n")
        for i, sentence in enumerate(sentences, 1):
            print(f"{i}. {sentence}")
    else:
        print("æ²¡æœ‰æ‰¾åˆ°å¥å­æˆ–æ–‡ä»¶è¯»å–å¤±è´¥")

if __name__ == "__main__":
    main() 