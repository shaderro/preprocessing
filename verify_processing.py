#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os

def verify_processing():
    """éªŒè¯5å¥è¯æ–‡ç« çš„å¤„ç†ç»“æœ"""
    
    print("ğŸ” éªŒè¯5å¥è¯æ–‡ç« çš„å¤„ç†ç»“æœ")
    print("=" * 50)
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    output_dir = "data/text_001"
    files_to_check = ["original_texts.json", "sentences.json", "tokens.json"]
    
    print("ğŸ“‹ æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
    print("-" * 30)
    
    for filename in files_to_check:
        file_path = os.path.join(output_dir, filename)
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"âœ… {filename}: {size:,} bytes")
        else:
            print(f"âŒ {filename}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # è¯»å–å¹¶åˆ†ææ•°æ®
    print(f"\nğŸ“Š æ•°æ®åˆ†æ")
    print("-" * 30)
    
    # è¯»å–original_texts.json
    with open(os.path.join(output_dir, "original_texts.json"), 'r', encoding='utf-8') as f:
        original_texts = json.load(f)
    
    print(f"ğŸ“„ æ–‡æœ¬ä¿¡æ¯:")
    print(f"   - æ–‡æœ¬ID: {original_texts['text_id']}")
    print(f"   - æ ‡é¢˜: {original_texts['text_title']}")
    print(f"   - å¥å­æ•°é‡: {len(original_texts['sentence_ids'])}")
    print(f"   - å¥å­IDåˆ—è¡¨: {original_texts['sentence_ids']}")
    
    # è¯»å–sentences.json
    with open(os.path.join(output_dir, "sentences.json"), 'r', encoding='utf-8') as f:
        sentences = json.load(f)
    
    print(f"\nğŸ“ å¥å­åˆ†æ:")
    print(f"   - å¥å­æ•°é‡: {len(sentences)}")
    total_tokens = 0
    for i, sentence in enumerate(sentences, 1):
        token_count = len(sentence['token_ids'])
        total_tokens += token_count
        print(f"   - å¥å­ {i}: {token_count} ä¸ªtokens")
        print(f"     å†…å®¹: {sentence['sentence_body'][:50]}...")
    
    # è¯»å–tokens.json
    with open(os.path.join(output_dir, "tokens.json"), 'r', encoding='utf-8') as f:
        tokens = json.load(f)
    
    print(f"\nğŸ”¤ Tokenåˆ†æ:")
    print(f"   - æ€»Tokenæ•°é‡: {len(tokens)}")
    print(f"   - è®¡ç®—Tokenæ•°é‡: {total_tokens}")
    
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    print(f"\nğŸ” æ•°æ®ä¸€è‡´æ€§éªŒè¯")
    print("-" * 30)
    
    # éªŒè¯å¥å­IDä¸€è‡´æ€§
    original_sentence_ids = set(original_texts['sentence_ids'])
    sentence_ids_from_sentences = set(s['sentence_id'] for s in sentences)
    
    if original_sentence_ids == sentence_ids_from_sentences:
        print("âœ… å¥å­IDä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    else:
        print("âŒ å¥å­IDä¸€è‡´æ€§éªŒè¯å¤±è´¥")
    
    # éªŒè¯token IDä¸€è‡´æ€§
    all_token_ids_from_sentences = set()
    for sentence in sentences:
        all_token_ids_from_sentences.update(sentence['token_ids'])
    
    token_ids_from_tokens = set(t['token_id'] for t in tokens)
    
    if all_token_ids_from_sentences == token_ids_from_tokens:
        print("âœ… Token IDä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    else:
        print("âŒ Token IDä¸€è‡´æ€§éªŒè¯å¤±è´¥")
    
    # éªŒè¯tokenæ•°é‡ä¸€è‡´æ€§
    if len(tokens) == total_tokens:
        print("âœ… Tokenæ•°é‡ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    else:
        print("âŒ Tokenæ•°é‡ä¸€è‡´æ€§éªŒè¯å¤±è´¥")
        print(f"   Tokensæ–‡ä»¶: {len(tokens)}")
        print(f"   è®¡ç®—æ€»æ•°: {total_tokens}")
    
    # æ˜¾ç¤ºä¸€äº›tokenç¤ºä¾‹
    print(f"\nğŸ“‹ Tokenç¤ºä¾‹")
    print("-" * 30)
    
    # æ˜¾ç¤ºå‰10ä¸ªtoken
    for i, token in enumerate(tokens[:10]):
        print(f"   {i+1:2d}. ID={token['token_id']:3d}, Body='{token['token_body']}', Type={token['token_type']}")
    
    # æ˜¾ç¤ºä¸åŒç±»å‹çš„tokenç»Ÿè®¡
    token_types = {}
    for token in tokens:
        token_type = token['token_type']
        token_types[token_type] = token_types.get(token_type, 0) + 1
    
    print(f"\nğŸ“Š Tokenç±»å‹ç»Ÿè®¡")
    print("-" * 30)
    for token_type, count in token_types.items():
        print(f"   {token_type}: {count} ä¸ª")
    
    print(f"\nğŸ‰ éªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    verify_processing() 