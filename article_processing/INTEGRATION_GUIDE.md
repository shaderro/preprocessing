# æ–‡ç« å¤„ç†æ¨¡å—é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•å°† `article_processing` æ¨¡å—é›†æˆåˆ°ä½ çš„ä¸»é¡¹ç›®ä¸­ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é›†æˆ `single token difficulty estimation` å’Œ `vocab explanation` åŠŸèƒ½ã€‚

## ğŸ”§ å½“å‰çŠ¶æ€

### âœ… å·²å®Œæˆçš„åŠŸèƒ½
- å¥å­åˆ†å‰² (`split_sentences`)
- Tokenåˆ†å‰² (`split_tokens`)
- åŸºæœ¬æ•°æ®ç»“æ„åˆ›å»º
- æ•°æ®ä¿å­˜åŠŸèƒ½

### âŒ éœ€è¦é›†æˆçš„åŠŸèƒ½
- Single Token Difficulty Estimation
- Vocab Explanation
- Lemma è·å–
- POS Tag æ ‡æ³¨

## ğŸš€ é›†æˆæ­¥éª¤

### æ­¥éª¤1: å¤åˆ¶æ¨¡å—åˆ°ä¸»é¡¹ç›®

å°† `article_processing` æ–‡ä»¶å¤¹å¤åˆ¶åˆ°ä½ çš„ä¸»é¡¹ç›®ä¸­ï¼š

```bash
cp -r article_processing /path/to/your/main/project/
```

### æ­¥éª¤2: ä¿®æ”¹å¯¼å…¥è·¯å¾„

åœ¨ `enhanced_processor.py` ä¸­ï¼Œéœ€è¦æ ¹æ®ä½ çš„ä¸»é¡¹ç›®ç»“æ„è°ƒæ•´å¯¼å…¥è·¯å¾„ï¼š

#### 2.1 éš¾åº¦è¯„ä¼°å™¨å¯¼å…¥

```python
# åœ¨ _init_difficulty_estimator æ–¹æ³•ä¸­
def _init_difficulty_estimator(self):
    """åˆå§‹åŒ–éš¾åº¦è¯„ä¼°å™¨"""
    try:
        # æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´è·¯å¾„
        from your_project.agents.single_token_difficulty_estimation import SingleTokenDifficultyEstimator
        self.difficulty_estimator = SingleTokenDifficultyEstimator()
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥éš¾åº¦è¯„ä¼°å™¨: {e}")
        self.enable_difficulty_estimation = False

def _init_lemma_processor(self):
    """åˆå§‹åŒ–lemmaå¤„ç†å™¨"""
    try:
        # æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´è·¯å¾„
        from your_project.utils.get_lemma import get_lemma
        self.lemma_processor = get_lemma
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥lemmaå¤„ç†å™¨: {e}")
        self.lemma_processor = None
```

#### 2.2 è¯æ±‡è½¬æ¢å™¨å¯¼å…¥

```python
# åœ¨ _init_vocab_converter æ–¹æ³•ä¸­
def _init_vocab_converter(self):
    """åˆå§‹åŒ–è¯æ±‡è½¬æ¢å™¨"""
    try:
        # æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´è·¯å¾„
        from your_project.utils.token_to_vocab import TokenToVocabConverter
        vocab_data_file = os.path.join(self.output_base_dir, "vocab_data.json")
        self.vocab_converter = TokenToVocabConverter(vocab_data_file)
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥è¯æ±‡è½¬æ¢å™¨: {e}")
        self.enable_vocab_explanation = False
```

#### 2.3 Lemma åŠŸèƒ½å¯¼å…¥

```python
# åœ¨ get_token_lemma æ–¹æ³•ä¸­ï¼ˆå·²é›†æˆåˆ°å¤„ç†å™¨ä¸­ï¼‰
def get_token_lemma(self, token_body: str) -> Optional[str]:
    """è·å–tokençš„lemmaå½¢å¼"""
    if not self.lemma_processor:
        return None
        
    try:
        # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œlemmaå¤„ç†
        if not token_body or not token_body.strip():
            return None
        
        # è°ƒç”¨lemmaå¤„ç†å™¨
        lemma = self.lemma_processor(token_body)
        return lemma
        
    except Exception as e:
        print(f"âŒ è·å–token '{token_body}' çš„lemmaæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None
```

### æ­¥éª¤3: ä½¿ç”¨å¢å¼ºç‰ˆå¤„ç†å™¨

```python
from article_processing.enhanced_processor import EnhancedArticleProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = EnhancedArticleProcessor(output_base_dir="data")

# å¯ç”¨é«˜çº§åŠŸèƒ½
processor.enable_advanced_features(
    enable_difficulty=True,  # å¯ç”¨éš¾åº¦è¯„ä¼°
    enable_vocab=True        # å¯ç”¨è¯æ±‡è§£é‡Š
)

# å¤„ç†æ–‡ç« 
result = processor.process_article_enhanced(
    raw_text="Your article text here...",
    text_id=1,
    text_title="My Article"
)

# ä¿å­˜æ•°æ®
processor.save_enhanced_data(result, "output_data")
```

## ğŸ“Š è¾“å‡ºæ ¼å¼

### åŸºç¡€è¾“å‡º
```json
{
  "text_id": 1,
  "text_title": "My Article",
  "sentences": [...],
  "total_sentences": 5,
  "total_tokens": 120
}
```

### å¢å¼ºè¾“å‡ºï¼ˆå¯ç”¨é«˜çº§åŠŸèƒ½åï¼‰
```json
{
  "text_id": 1,
  "text_title": "My Article",
  "sentences": [...],
  "total_sentences": 5,
  "total_tokens": 120,
  "vocab_expressions": [...]
}
```

### Token å¢å¼ºå­—æ®µ
```json
{
  "token_body": "artificial",
  "token_type": "text",
  "difficulty_level": "hard",
  "global_token_id": 0,
  "sentence_token_id": 1,
  "linked_vocab_id": 1,
  "pos_tag": "JJ",
  "lemma": "artificial",
  "is_grammar_marker": false
}
```

## ğŸ”— ä¾èµ–å…³ç³»

### å¿…éœ€ä¾èµ–
- `src/agents/single_token_difficulty_estimation.py`
- `src/utils/token_to_vocab.py`
- `src/utils/get_lemma.py`
- `src/core/token_data.py` (å·²å­˜åœ¨)

### å¯é€‰ä¾èµ–
- `src/utils/get_pos_tag.py` (ç”¨äºPOSæ ‡æ³¨)
- `src/agents/grammar_analysis.py` (ç”¨äºè¯­æ³•åˆ†æ)

## ğŸ§ª æµ‹è¯•é›†æˆ

### æµ‹è¯•è„šæœ¬

```python
# test_integration.py
from article_processing.enhanced_processor import EnhancedArticleProcessor

def test_integration():
    # åˆ›å»ºå¤„ç†å™¨
    processor = EnhancedArticleProcessor()
    
    # å¯ç”¨æ‰€æœ‰åŠŸèƒ½
    processor.enable_advanced_features(True, True)
    
    # æµ‹è¯•æ–‡ç« 
    test_article = """
    Artificial intelligence has revolutionized the way we interact with technology. 
    Machine learning algorithms can now process vast amounts of data efficiently.
    """
    
    # å¤„ç†æ–‡ç« 
    result = processor.process_article_enhanced(
        raw_text=test_article,
        text_id=1,
        text_title="Test Article"
    )
    
    # éªŒè¯ç»“æœ
    print(f"å¥å­æ•°: {result['total_sentences']}")
    print(f"Tokenæ•°: {result['total_tokens']}")
    print(f"è¯æ±‡è§£é‡Šæ•°: {len(result.get('vocab_expressions', []))}")
    
    # æ£€æŸ¥éš¾åº¦è¯„ä¼°
    hard_tokens = 0
    for sentence in result['sentences']:
        for token in sentence['tokens']:
            if token.get('difficulty_level') == 'hard':
                hard_tokens += 1
                print(f"Hard token: {token['token_body']}")
    
    print(f"Hard tokens: {hard_tokens}")

if __name__ == "__main__":
    test_integration()
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å¯¼å…¥è·¯å¾„
ç¡®ä¿æ‰€æœ‰å¯¼å…¥è·¯å¾„éƒ½æ­£ç¡®æŒ‡å‘ä½ çš„ä¸»é¡¹ç›®ç»“æ„ã€‚

### 2. ä¾èµ–æ£€æŸ¥
åœ¨å¯ç”¨é«˜çº§åŠŸèƒ½å‰ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…å’Œé…ç½®ã€‚

### 3. é”™è¯¯å¤„ç†
æ¨¡å—åŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œå¦‚æœæŸä¸ªåŠŸèƒ½æ— æ³•åŠ è½½ï¼Œä¼šè‡ªåŠ¨ç¦ç”¨è¯¥åŠŸèƒ½å¹¶ç»§ç»­å¤„ç†ã€‚

### 4. æ€§èƒ½è€ƒè™‘
- éš¾åº¦è¯„ä¼°å’Œè¯æ±‡è§£é‡Šä¼šå¢åŠ å¤„ç†æ—¶é—´
- å»ºè®®å¯¹å¤§å‹æ–‡æ¡£è¿›è¡Œåˆ†æ‰¹å¤„ç†
- å¯ä»¥è€ƒè™‘æ·»åŠ ç¼“å­˜æœºåˆ¶

## ğŸ”„ è¿ç§»æ£€æŸ¥æ¸…å•

- [ ] å¤åˆ¶ `article_processing` æ–‡ä»¶å¤¹åˆ°ä¸»é¡¹ç›®
- [ ] ä¿®æ”¹ `enhanced_processor.py` ä¸­çš„å¯¼å…¥è·¯å¾„
- [ ] ç¡®ä¿æ‰€æœ‰ä¾èµ–æ¨¡å—å¯ç”¨
- [ ] æµ‹è¯•åŸºç¡€åŠŸèƒ½ï¼ˆå¥å­åˆ†å‰²ã€tokenåˆ†å‰²ï¼‰
- [ ] æµ‹è¯•éš¾åº¦è¯„ä¼°åŠŸèƒ½
- [ ] æµ‹è¯•è¯æ±‡è§£é‡ŠåŠŸèƒ½
- [ ] éªŒè¯è¾“å‡ºæ ¼å¼
- [ ] é›†æˆåˆ°ä¸»é¡¹ç›®çš„å·¥ä½œæµç¨‹ä¸­

## ğŸ“ æ”¯æŒ

å¦‚æœåœ¨é›†æˆè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. å¯¼å…¥è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ä¾èµ–æ¨¡å—æ˜¯å¦å¯ç”¨
3. é¡¹ç›®ç»“æ„æ˜¯å¦åŒ¹é…
4. é”™è¯¯æ—¥å¿—ä¸­çš„å…·ä½“ä¿¡æ¯ 