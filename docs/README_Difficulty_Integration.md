# éš¾åº¦è¯„ä¼°å’ŒLemmaé›†æˆè¯´æ˜

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜äº†åœ¨ `text_processor.py` ä¸­é›†æˆ `get_difficulty_agent` å’Œ `get_lemma` åŠŸèƒ½çš„å…·ä½“å®ç°ã€‚

## ğŸ”§ é›†æˆåŠŸèƒ½

### 1. æ ¸å¿ƒåŠŸèƒ½

- âœ… è‡ªåŠ¨è¯†åˆ«tokenç±»å‹
- âœ… åªå¯¹ `token_type` ä¸º "text" çš„tokenè¿›è¡Œéš¾åº¦è¯„ä¼°å’Œlemmaå¤„ç†
- âœ… ä¿æŒå…¶ä»–tokenç±»å‹ï¼ˆspaceã€punctuationï¼‰çš„ `difficulty_level` å’Œ `lemma` ä¸º `null`
- âœ… é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ•è·
- âœ… ç»“æœæ ¼å¼éªŒè¯

### 2. å®ç°ç»†èŠ‚

#### 2.1 å¯¼å…¥ä¾èµ–

```python
from single_token_difficulty_estimation import SingleTokenDifficultyEstimator
from get_lemma import get_lemma
```

#### 2.2 åˆå§‹åŒ–å¤„ç†å™¨

```python
def __init__(self, output_base_dir: str = "data"):
    self.output_base_dir = output_base_dir
    os.makedirs(output_base_dir, exist_ok=True)
    # åˆå§‹åŒ–éš¾åº¦è¯„ä¼°å™¨
    self.difficulty_estimator = SingleTokenDifficultyEstimator()
```

#### 2.3 éš¾åº¦è¯„ä¼°æ–¹æ³•

```python
def assess_token_difficulty(self, token_body: str, context: str = "") -> str:
    """
    è¯„ä¼°tokençš„éš¾åº¦çº§åˆ«
    
    Args:
        token_body: tokenå†…å®¹
        context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: éš¾åº¦çº§åˆ« ("easy" æˆ– "hard")
    """
    try:
        # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œéš¾åº¦è¯„ä¼°
        if not token_body or not token_body.strip():
            return None
        
        # è°ƒç”¨éš¾åº¦è¯„ä¼°å™¨
        difficulty_result = self.difficulty_estimator.run(token_body, verbose=False)
        
        # æ¸…ç†ç»“æœï¼Œç¡®ä¿åªè¿”å› "easy" æˆ– "hard"
        difficulty_result = difficulty_result.strip().lower()
        if difficulty_result in ["easy", "hard"]:
            return difficulty_result
        else:
            # å¦‚æœç»“æœä¸æ˜¯é¢„æœŸçš„æ ¼å¼ï¼Œè¿”å›é»˜è®¤å€¼
            print(f"âš ï¸  è­¦å‘Šï¼štoken '{token_body}' çš„éš¾åº¦è¯„ä¼°ç»“æœæ ¼å¼å¼‚å¸¸: '{difficulty_result}'")
            return "easy"  # é»˜è®¤è¿”å›easy
            
    except Exception as e:
        print(f"âŒ è¯„ä¼°token '{token_body}' éš¾åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None
```

#### 2.4 Lemmaå¤„ç†æ–¹æ³•

```python
def get_token_lemma(self, token_body: str) -> str:
    """
    è·å–tokençš„lemmaå½¢å¼
    
    Args:
        token_body: tokenå†…å®¹
        
    Returns:
        str: lemmaå½¢å¼ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›None
    """
    try:
        # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œlemmaå¤„ç†
        if not token_body or not token_body.strip():
            return None
        
        # è°ƒç”¨get_lemmaå‡½æ•°
        lemma = get_lemma(token_body)
        return lemma
        
    except Exception as e:
        print(f"âŒ è·å–token '{token_body}' çš„lemmaæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None
```

#### 2.5 é›†æˆåˆ°å¤„ç†æµç¨‹

```python
# åœ¨ process_text_to_structured_data æ–¹æ³•ä¸­
for token_id, token_dict in enumerate(token_dicts, 1):
    # è¯„ä¼°éš¾åº¦çº§åˆ«å’Œè·å–lemmaï¼ˆåªå¯¹textç±»å‹çš„tokenï¼‰
    difficulty_level = None
    lemma = None
    if token_dict["token_type"] == "text":
        difficulty_level = self.assess_token_difficulty(token_dict["token_body"], sentence_text)
        lemma = self.get_token_lemma(token_dict["token_body"])
    
    token = Token(
        token_body=token_dict["token_body"],
        token_type=token_dict["token_type"],
        global_token_id=global_token_id,
        sentence_token_id=token_id,
        difficulty_level=difficulty_level,
        lemma=lemma
    )
    tokens.append(token)
    global_token_id += 1
```

## ğŸ“Š æµ‹è¯•ç»“æœ

### æµ‹è¯•ç”¨ä¾‹

ä½¿ç”¨æµ‹è¯•æ–‡æœ¬ï¼š`"Artificial intelligence has revolutionized the way we interact with technology."`

### ç»“æœåˆ†æ

```
ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:
   æ€»tokenæ•°é‡: 20
   textç±»å‹token: 10
   æœ‰lemmaçš„token: 10
   æ— lemmaçš„token: 10

âœ… éªŒè¯ç»“æœ:
âœ… æ‰€æœ‰textç±»å‹tokenéƒ½æœ‰lemma
âœ… æ‰€æœ‰étextç±»å‹tokenéƒ½æ²¡æœ‰lemma
```

### å…·ä½“ç¤ºä¾‹

```json
{
  "token_id": 0,
  "sentence_id": 1,
  "token_body": "Artificial",
  "token_type": "text",
  "sentence_token_index": 0,
  "difficulty_level": "hard",
  "explanation": null,
  "pos_tag": null,
  "lemma": "artificial",
  "is_grammar_marker": false
},
{
  "token_id": 1,
  "sentence_id": 1,
  "token_body": " ",
  "token_type": "space",
  "sentence_token_index": 1,
  "difficulty_level": null,
  "explanation": null,
  "pos_tag": null,
  "lemma": null,
  "is_grammar_marker": false
}
```

## ğŸ¯ å…³é”®ç‰¹æ€§

### 1. ç±»å‹è¯†åˆ«

- **text**: è¿›è¡Œéš¾åº¦è¯„ä¼°å’Œlemmaå¤„ç†ï¼Œè¿”å›ç›¸åº”çš„å€¼
- **space**: ä¿æŒ `difficulty_level` å’Œ `lemma` ä¸º `null`
- **punctuation**: ä¿æŒ `difficulty_level` å’Œ `lemma` ä¸º `null`

### 2. é”™è¯¯å¤„ç†

- ç©ºtokenæˆ–ç©ºç™½tokenï¼šè¿”å› `None`
- APIè°ƒç”¨å¤±è´¥ï¼šè¿”å› `None` å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯
- æ ¼å¼å¼‚å¸¸ï¼šè¿”å›é»˜è®¤å€¼å¹¶æ‰“å°è­¦å‘Š
- NLTKæ•°æ®ç¼ºå¤±ï¼šè‡ªåŠ¨ä¸‹è½½å¿…è¦æ•°æ®

### 3. æ€§èƒ½ä¼˜åŒ–

- åªå¯¹å¿…è¦çš„tokenè¿›è¡ŒAPIè°ƒç”¨
- é¿å…é‡å¤è¯„ä¼°
- å¼‚å¸¸æƒ…å†µä¸‹æœ‰åˆç†çš„é»˜è®¤å€¼
- é™é»˜å¤„ç†NLTKé”™è¯¯

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from text_processor import TextProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = TextProcessor()

# å¤„ç†æ–‡æœ¬
original_text = processor.process_text_to_structured_data(
    "Your text here", 
    text_id=1, 
    text_title="æµ‹è¯•æ–‡æœ¬"
)

# æŸ¥çœ‹ç»“æœ
for sentence in original_text.text_by_sentence:
    for token in sentence.tokens:
        if token.token_type == "text":
            print(f"Token: {token.token_body}")
            print(f"  Difficulty: {token.difficulty_level}")
            print(f"  Lemma: {token.lemma}")
```

### 2. æ–‡ä»¶å¤„ç†

```python
# å¤„ç†æ–‡ä»¶
success = processor.process_file("input.txt", text_id=1)
if success:
    print("å¤„ç†å®Œæˆï¼")
```

## ğŸ” éªŒè¯æ–¹æ³•

è¿è¡ŒéªŒè¯è„šæœ¬ï¼š

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è¿è¡Œæµ‹è¯•
python test_difficulty_integration.py
python test_lemma_integration.py

# è¿è¡Œå®Œæ•´éªŒè¯
python verify_processing.py
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **APIä¾èµ–**: éœ€è¦å®‰è£… `openai` æ¨¡å—
2. **NLTKä¾èµ–**: éœ€è¦å®‰è£… `nltk` æ¨¡å—å’Œç›¸å…³æ•°æ®
3. **ç½‘ç»œè¿æ¥**: éœ€è¦ç½‘ç»œè¿æ¥æ¥è°ƒç”¨AI APIå’Œä¸‹è½½NLTKæ•°æ®
4. **APIé™åˆ¶**: æ³¨æ„APIè°ƒç”¨é¢‘ç‡å’Œé…é¢é™åˆ¶
5. **é”™è¯¯å¤„ç†**: å¼‚å¸¸æƒ…å†µä¸‹ä¼šæœ‰åˆç†çš„é»˜è®¤å€¼
6. **æ€§èƒ½è€ƒè™‘**: å¤§é‡æ–‡æœ¬å¤„ç†æ—¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´

## ğŸ‰ æ€»ç»“

æˆåŠŸé›†æˆäº† `get_difficulty_agent` å’Œ `get_lemma` åŠŸèƒ½åˆ° `text_processor.py` ä¸­ï¼Œå®ç°äº†ï¼š

- âœ… è‡ªåŠ¨tokenç±»å‹è¯†åˆ«
- âœ… æ™ºèƒ½éš¾åº¦è¯„ä¼°
- âœ… å‡†ç¡®çš„lemmaå¤„ç†
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†
- âœ… æ•°æ®ä¸€è‡´æ€§ä¿è¯
- âœ… è¯¦ç»†çš„æµ‹è¯•éªŒè¯

é›†æˆåçš„ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨ä¸ºæ–‡æœ¬tokenæä¾›éš¾åº¦è¯„ä¼°å’Œlemmaå¤„ç†ï¼ŒåŒæ—¶ä¿æŒæ•°æ®ç»“æ„çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚ 