#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import json
import os
import sys
from typing import List, Union, Dict, Any, Optional
from dataclasses import dataclass, asdict
from .token_data import OriginalText, Sentence, Token, VocabExpression, VocabExpressionExample
from ..agents.single_token_difficulty_estimation import SingleTokenDifficultyEstimator
from ..utils.get_lemma import get_lemma

class TextProcessor:
    """æ–‡æœ¬å¤„ç†å™¨ï¼šå°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆç»“æ„åŒ–æ•°æ®"""
    
    def __init__(self, output_base_dir: str = "data"):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        
        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_base_dir = output_base_dir
        os.makedirs(output_base_dir, exist_ok=True)
        # åˆå§‹åŒ–éš¾åº¦è¯„ä¼°å™¨
        self.difficulty_estimator = SingleTokenDifficultyEstimator()
        # åˆå§‹åŒ–vocabè½¬æ¢å™¨
        self.vocab_converter = None
        self.vocab_counter = 1
        
    def _init_vocab_converter(self, vocab_data_file: str = None):
        """åˆå§‹åŒ–vocabè½¬æ¢å™¨"""
        if self.vocab_converter is None:
            if vocab_data_file is None:
                vocab_data_file = os.path.join(self.output_base_dir, "vocab_data.json")
            from ..utils.token_to_vocab import TokenToVocabConverter
            self.vocab_converter = TokenToVocabConverter(vocab_data_file)
            self.vocab_counter = self.vocab_converter.vocab_counter
    
    def split_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬æŒ‰å¥å­åˆ†éš”
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å¹¶å»é™¤é¦–å°¾ç©ºç™½
        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
        
        return sentences
    
    def split_tokens(self, text: str) -> List[Dict[str, Any]]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆtokens
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[Dict[str, Any]]: tokenåˆ—è¡¨
        """
        if not text:
            return []
        
        tokens = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸åŒç±»å‹çš„token
        word_pattern = r'\b[\w\'-]+\b'
        punctuation_pattern = r'[^\w\s]'
        space_pattern = r'\s+'
        
        # ç»„åˆæ‰€æœ‰æ¨¡å¼
        combined_pattern = f'({word_pattern})|({punctuation_pattern})|({space_pattern})'
        
        matches = re.finditer(combined_pattern, text)
        
        for match in matches:
            token_body = match.group(0)
            
            # ç¡®å®štokenç±»å‹
            if match.group(1):  # å•è¯
                token_type = "text"
            elif match.group(2):  # æ ‡ç‚¹ç¬¦å·
                token_type = "punctuation"
            elif match.group(3):  # ç©ºç™½å­—ç¬¦
                token_type = "space"
            else:
                continue
            
            token = {
                "token_body": token_body,
                "token_type": token_type
            }
            
            tokens.append(token)
        
        return tokens
    
    def assess_token_difficulty(self, token_body: str, context: str = "") -> str:
        """
        è¯„ä¼°tokençš„éš¾åº¦çº§åˆ«
        
        Args:
            token_body: tokenå†…å®¹
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: éš¾åº¦çº§åˆ« ("easy" æˆ– "hard")
        """
        try:
            # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œéš¾åº¦è¯„ä¼°
            if not token_body or not token_body.strip():
                return None
            
            # è°ƒç”¨éš¾åº¦è¯„ä¼°å™¨
            difficulty_result = self.difficulty_estimator.run(token_body, verbose=False)
            
            # æ¸…ç†ç»“æœï¼Œç¡®ä¿åªè¿”å› "easy" æˆ– "hard"
            difficulty_result = difficulty_result.strip().lower()
            if difficulty_result in ["easy", "hard"]:
                return difficulty_result
            else:
                # å¦‚æœç»“æœä¸æ˜¯é¢„æœŸçš„æ ¼å¼ï¼Œè¿”å›é»˜è®¤å€¼
                print(f"âš ï¸  è­¦å‘Šï¼štoken '{token_body}' çš„éš¾åº¦è¯„ä¼°ç»“æœæ ¼å¼å¼‚å¸¸: '{difficulty_result}'")
                return "easy"  # é»˜è®¤è¿”å›easy
                
        except Exception as e:
            print(f"âŒ è¯„ä¼°token '{token_body}' éš¾åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def get_token_lemma(self, token_body: str) -> str:
        """
        è·å–tokençš„lemmaå½¢å¼
        
        Args:
            token_body: tokenå†…å®¹
            
        Returns:
            str: lemmaå½¢å¼ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›None
        """
        try:
            # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œlemmaå¤„ç†
            if not token_body or not token_body.strip():
                return None
            
            # è°ƒç”¨get_lemmaå‡½æ•°
            lemma = get_lemma(token_body)
            return lemma
            
        except Exception as e:
            print(f"âŒ è·å–token '{token_body}' çš„lemmaæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def process_text_to_structured_data(self, text: Union[str, str], text_id: int, text_title: str = "") -> OriginalText:
        """
        å°†æ–‡æœ¬å¤„ç†æˆç»“æ„åŒ–æ•°æ®
        
        Args:
            text: æ–‡æœ¬å†…å®¹æˆ–æ–‡ä»¶è·¯å¾„
            text_id: æ–‡æœ¬ID
            text_title: æ–‡æœ¬æ ‡é¢˜
            
        Returns:
            OriginalText: ç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®
        """
        # å¦‚æœè¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå…ˆè¯»å–æ–‡ä»¶
        if os.path.isfile(text):
            with open(text, 'r', encoding='utf-8') as file:
                text_content = file.read()
            if not text_title:
                text_title = os.path.basename(text)
        else:
            text_content = text
            if not text_title:
                text_title = f"Text_{text_id}"
        
        # åˆå§‹åŒ–vocabè½¬æ¢å™¨
        self._init_vocab_converter()
        
        # åˆ†å‰²å¥å­
        sentence_texts = self.split_sentences(text_content)
        
        # åˆ›å»ºå¥å­å¯¹è±¡åˆ—è¡¨
        sentences = []
        global_token_id = 0
        vocab_expressions = []  # å­˜å‚¨ç”Ÿæˆçš„vocab
        
        for sentence_id, sentence_text in enumerate(sentence_texts, 1):
            # åˆ†å‰²tokens
            token_dicts = self.split_tokens(sentence_text)
            
            # åˆ›å»ºTokenå¯¹è±¡åˆ—è¡¨
            tokens = []
            for token_id, token_dict in enumerate(token_dicts, 1):
                # è¯„ä¼°éš¾åº¦çº§åˆ«å’Œè·å–lemmaï¼ˆåªå¯¹textç±»å‹çš„tokenï¼‰
                difficulty_level = None
                lemma = None
                if token_dict["token_type"] == "text":
                    difficulty_level = self.assess_token_difficulty(token_dict["token_body"], sentence_text)
                    lemma = self.get_token_lemma(token_dict["token_body"])
                
                token = Token(
                    token_body=token_dict["token_body"],
                    token_type=token_dict["token_type"],
                    global_token_id=global_token_id,
                    sentence_token_id=token_id,
                    difficulty_level=difficulty_level,
                    lemma=lemma,
                    linked_vocab_id=None  # åˆå§‹åŒ–ä¸ºNoneï¼Œç¨åæ›´æ–°
                )
                tokens.append(token)
                global_token_id += 1
            
            # åˆ›å»ºSentenceå¯¹è±¡
            sentence = Sentence(
                text_id=text_id,
                sentence_id=sentence_id,
                sentence_body=sentence_text,
                grammar_annotations=[],
                vocab_annotations=[],
                tokens=tokens
            )
            sentences.append(sentence)
            
            # ä¸ºhardéš¾åº¦çš„tokenç”Ÿæˆvocab
            for token in tokens:
                if token.token_type == "text" and token.difficulty_level == "hard":
                    vocab = self._generate_vocab_for_token(token, sentence, text_id)
                    if vocab:
                        vocab_expressions.append(vocab)
                        # æ›´æ–°tokençš„linked_vocab_id
                        token.linked_vocab_id = vocab.vocab_id
        
        # åˆ›å»ºOriginalTextå¯¹è±¡
        original_text = OriginalText(
            text_id=text_id,
            text_title=text_title,
            text_by_sentence=sentences
        )
        
        # ä¿å­˜vocabæ•°æ®
        if vocab_expressions:
            self._save_vocab_data(vocab_expressions)
        
        return original_text
    
    def save_structured_data(self, original_text: OriginalText, output_dir: str):
        """
        ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æŒ‡å®šç›®å½•ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…é‡å¤å†—ä½™ï¼‰
        
        Args:
            original_text: ç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ original_texts.jsonï¼ˆåªä¿ç•™æ•´ä½“æ–‡æœ¬å’Œ metadataï¼‰
        original_text_data = {
            "text_id": original_text.text_id,
            "text_title": original_text.text_title,
            "text_body": "\n".join([sentence.sentence_body for sentence in original_text.text_by_sentence]),
            "sentence_ids": [sentence.sentence_id for sentence in original_text.text_by_sentence]
        }
        
        with open(os.path.join(output_dir, "original_texts.json"), 'w', encoding='utf-8') as f:
            json.dump(original_text_data, f, ensure_ascii=False, indent=2)
        
        # 2. ä¿å­˜ sentences.jsonï¼ˆæŒ‰ sentence æ‹†åˆ†ï¼Œé¿å…é‡å¤ text_idï¼‰
        sentences_data = []
        for sentence in original_text.text_by_sentence:
            # æ”¶é›†è¯¥å¥å­çš„æ‰€æœ‰token IDs
            token_ids = [token.global_token_id for token in sentence.tokens]
            
            sentence_data = {
                "sentence_id": sentence.sentence_id,
                "text_id": sentence.text_id,
                "sentence_body": sentence.sentence_body,
                "token_ids": token_ids,
                "grammar_annotations": sentence.grammar_annotations,
                "vocab_annotations": sentence.vocab_annotations
            }
            sentences_data.append(sentence_data)
        
        with open(os.path.join(output_dir, "sentences.json"), 'w', encoding='utf-8') as f:
            json.dump(sentences_data, f, ensure_ascii=False, indent=2)
        
        # 3. ä¿å­˜ tokens.jsonï¼ˆç‹¬ç«‹ä¿å­˜æ‰€æœ‰ token ä¿¡æ¯ï¼Œæä¾›å…¨å±€ç´¢å¼•ï¼‰
        all_tokens = []
        for sentence in original_text.text_by_sentence:
            for token_index, token in enumerate(sentence.tokens):
                token_data = {
                    "text_id": sentence.text_id,
                    "token_id": token.global_token_id,
                    "sentence_id": sentence.sentence_id,
                    "token_body": token.token_body,
                    "token_type": token.token_type,
                    "sentence_token_index": token_index,
                    "difficulty_level": token.difficulty_level,
                    "linked_vocab_id": token.linked_vocab_id,
                    "pos_tag": token.pos_tag,
                    "lemma": token.lemma,
                    "is_grammar_marker": token.is_grammar_marker
                }
                all_tokens.append(token_data)
        
        with open(os.path.join(output_dir, "tokens.json"), 'w', encoding='utf-8') as f:
            json.dump(all_tokens, f, ensure_ascii=False, indent=2)
    
    def process_file(self, input_path: str, text_id: int, output_dir: str = None) -> bool:
        """
        å¤„ç†æ–‡æœ¬æ–‡ä»¶å¹¶ä¿å­˜ç»“æ„åŒ–æ•°æ®
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            text_id: æ–‡æœ¬ID
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if output_dir is None:
                output_dir = os.path.join(self.output_base_dir, f"text_{text_id:03d}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(input_path):
                print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{input_path}'")
                return False
            
            # å¤„ç†æ–‡æœ¬
            original_text = self.process_text_to_structured_data(input_path, text_id)
            
            # ä¿å­˜æ•°æ®
            self.save_structured_data(original_text, output_dir)
            
            print(f"âœ… æ–‡æœ¬å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"ğŸ“„ å¥å­æ•°é‡: {len(original_text.text_by_sentence)}")
            print(f"ğŸ”¤ æ€»tokenæ•°é‡: {sum(len(sentence.tokens) for sentence in original_text.text_by_sentence)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            return False
    
    def process_multiple_files(self, input_files: List[str], start_text_id: int = 1) -> int:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            start_text_id: èµ·å§‹æ–‡æœ¬ID
            
        Returns:
            int: æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°é‡
        """
        success_count = 0
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(input_files)} ä¸ªæ–‡ä»¶...")
        print("=" * 50)
        
        for i, file_path in enumerate(input_files, start_text_id):
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i-start_text_id+1}/{len(input_files)}: {file_path}")
            
            if self.process_file(file_path, i):
                success_count += 1
            else:
                print(f"âš ï¸  è·³è¿‡æ–‡ä»¶: {file_path}")
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(input_files)} ä¸ªæ–‡ä»¶")
        return success_count

    def _generate_vocab_for_token(self, token: Token, sentence: Sentence, text_id: int) -> Optional[VocabExpression]:
        """
        ä¸ºå•ä¸ªtokenç”Ÿæˆvocab
        
        Args:
            token: Tokenå¯¹è±¡
            sentence: å¥å­å¯¹è±¡
            text_id: æ–‡æœ¬ID
            
        Returns:
            VocabExpression: ç”Ÿæˆçš„vocabå¯¹è±¡ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥è¿”å›None
        """
        # æ£€æŸ¥tokenæ˜¯å¦ä¸ºhardéš¾åº¦çš„textç±»å‹
        if not (token.token_type == "text" and token.difficulty_level == "hard"):
            return None
        
        try:
            # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
            from ..agents import VocabExplanationAssistant, VocabExampleExplanationAssistant
            
            # åˆå§‹åŒ–åŠ©æ‰‹
            vocab_explanation_assistant = VocabExplanationAssistant()
            vocab_example_assistant = VocabExampleExplanationAssistant()
            
            # è·å–è¯æ±‡è§£é‡Š
            vocab_explanation_result = vocab_explanation_assistant.run(sentence, token.token_body)
            
            # è·å–ä¸Šä¸‹æ–‡è§£é‡Š
            context_explanation_result = vocab_example_assistant.run(token.token_body, sentence)
            
            # è§£æè§£é‡Šç»“æœ
            explanation = self._parse_explanation(vocab_explanation_result)
            context_explanation = self._parse_context_explanation(context_explanation_result)
            
            # åˆ›å»ºVocabExpressionå¯¹è±¡
            vocab_expression = VocabExpression(
                vocab_id=self.vocab_counter,
                vocab_body=token.lemma if token.lemma else token.token_body,  # ä½¿ç”¨lemma
                explanation=explanation,
                source="auto",  # æ ‡æ³¨ä¸ºauto
                is_starred=False,
                examples=[]
            )
            
            # åˆ›å»ºVocabExpressionExample
            if context_explanation:
                vocab_example = VocabExpressionExample(
                    vocab_id=self.vocab_counter,
                    text_id=text_id,
                    sentence_id=sentence.sentence_id,
                    context_explanation=context_explanation,
                    token_indices=[token.sentence_token_id] if token.sentence_token_id else []
                )
                vocab_expression.examples.append(vocab_example)
            
            # æ›´æ–°è®¡æ•°å™¨
            self.vocab_counter += 1
            
            return vocab_expression
            
        except Exception as e:
            print(f"è½¬æ¢token '{token.token_body}' åˆ°vocabå¤±è´¥: {e}")
            return None
    
    def _parse_explanation(self, result: Any) -> str:
        """è§£æè¯æ±‡è§£é‡Šç»“æœ"""
        if isinstance(result, dict):
            return result.get('explanation', '')
        elif isinstance(result, str):
            # å°è¯•è§£æJSONå­—ç¬¦ä¸²
            try:
                data = json.loads(result)
                return data.get('explanation', '')
            except:
                return result
        return str(result)
    
    def _parse_context_explanation(self, result: Any) -> str:
        """è§£æä¸Šä¸‹æ–‡è§£é‡Šç»“æœ"""
        if isinstance(result, dict):
            return result.get('explanation', '')
        elif isinstance(result, str):
            # å°è¯•è§£æJSONå­—ç¬¦ä¸²
            try:
                data = json.loads(result)
                return data.get('explanation', '')
            except:
                return result
        return str(result)

    def _save_vocab_data(self, vocab_expressions: List[VocabExpression]):
        """
        ä¿å­˜vocabæ•°æ®åˆ°æŒ‡å®šè·¯å¾„
        
        Args:
            vocab_expressions: vocabè¡¨è¾¾å¼åˆ—è¡¨
        """
        try:
            # åˆ›å»ºvocabæ•°æ®ç›®å½•
            vocab_output_dir = os.path.join(self.output_base_dir, "vocab_data")
            os.makedirs(vocab_output_dir, exist_ok=True)
            
            # è¯»å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            vocab_data_file = os.path.join(vocab_output_dir, "vocab_data.json")
            existing_vocabs = []
            if os.path.exists(vocab_data_file):
                try:
                    with open(vocab_data_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        existing_vocabs = existing_data.get('vocab_expressions', [])
                except Exception as e:
                    print(f"è¯»å–ç°æœ‰vocabæ•°æ®å¤±è´¥: {e}")
            
            # æ·»åŠ æ–°çš„vocabæ•°æ®
            for vocab in vocab_expressions:
                vocab_dict = {
                    'vocab_id': vocab.vocab_id,
                    'vocab_body': vocab.vocab_body,
                    'explanation': vocab.explanation,
                    'source': vocab.source,
                    'is_starred': vocab.is_starred,
                    'examples': [
                        {
                            'vocab_id': example.vocab_id,
                            'text_id': example.text_id,
                            'sentence_id': example.sentence_id,
                            'context_explanation': example.context_explanation,
                            'token_indices': example.token_indices
                        }
                        for example in vocab.examples
                    ]
                }
                existing_vocabs.append(vocab_dict)
            
            # ä¿å­˜vocabæ•°æ®
            vocab_data = {
                'vocab_expressions': existing_vocabs,
                'next_vocab_id': self.vocab_counter
            }
            
            with open(vocab_data_file, 'w', encoding='utf-8') as f:
                json.dump(vocab_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æˆåŠŸä¿å­˜ {len(vocab_expressions)} ä¸ªvocabåˆ° {vocab_data_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜vocabæ•°æ®å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†å‘½ä»¤è¡Œè¾“å…¥çš„æ–‡ä»¶"""
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python text_processor.py <æ–‡ä»¶è·¯å¾„1> [æ–‡ä»¶è·¯å¾„2] [æ–‡ä»¶è·¯å¾„3] ...")
        print("ç¤ºä¾‹: python text_processor.py input.txt")
        print("ç¤ºä¾‹: python text_processor.py file1.txt file2.txt file3.txt")
        sys.exit(1)
    
    # è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    input_files = sys.argv[1:]
    
    # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor()
    
    # å¤„ç†æ–‡ä»¶
    if len(input_files) == 1:
        # å•ä¸ªæ–‡ä»¶å¤„ç†
        success = processor.process_file(input_files[0], 1)
        if not success:
            sys.exit(1)
    else:
        # æ‰¹é‡å¤„ç†
        success_count = processor.process_multiple_files(input_files)
        if success_count == 0:
            sys.exit(1)

if __name__ == "__main__":
    main() 