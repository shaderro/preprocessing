#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.text_processor import TextProcessor

def test_lemma_integration():
    """æµ‹è¯•lemmaé›†æˆåŠŸèƒ½"""
    
    print("ğŸ” æµ‹è¯•lemmaé›†æˆåŠŸèƒ½")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = "Artificial intelligence has revolutionized the way we interact with technology."
    
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
    print("-" * 30)
    
    # å¤„ç†æ–‡æœ¬
    original_text = processor.process_text_to_structured_data(test_text, 999, "æµ‹è¯•æ–‡æœ¬")
    
    # åˆ†æç»“æœ
    print("ğŸ“Š åˆ†æç»“æœ:")
    print("-" * 30)
    
    total_tokens = 0
    text_tokens = 0
    lemma_tokens = 0
    null_lemma_tokens = 0
    
    for sentence in original_text.text_by_sentence:
        print(f"\nğŸ“„ å¥å­ {sentence.sentence_id}: {sentence.sentence_body}")
        print("   Tokens:")
        
        for token in sentence.tokens:
            total_tokens += 1
            lemma = token.lemma
            
            if token.token_type == "text":
                text_tokens += 1
                if lemma is not None:
                    lemma_tokens += 1
                    lemma_mark = "ğŸ“"
                else:
                    null_lemma_tokens += 1
                    lemma_mark = "âŒ"
            else:
                lemma_mark = "âšª"
                if lemma is None:
                    null_lemma_tokens += 1
                else:
                    print(f"âš ï¸  è­¦å‘Šï¼šétextç±»å‹token '{token.token_body}' æœ‰lemma: {lemma}")
            
            print(f"     {lemma_mark} '{token.token_body}' ({token.token_type}) -> Lemma: {lemma}")
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print("-" * 30)
    print(f"   æ€»tokenæ•°é‡: {total_tokens}")
    print(f"   textç±»å‹token: {text_tokens}")
    print(f"   æœ‰lemmaçš„token: {lemma_tokens}")
    print(f"   æ— lemmaçš„token: {null_lemma_tokens}")
    
    # éªŒè¯è§„åˆ™
    print(f"\nâœ… éªŒè¯ç»“æœ:")
    print("-" * 30)
    
    # éªŒè¯1: åªæœ‰textç±»å‹çš„tokenæœ‰lemma
    text_tokens_with_lemma = lemma_tokens
    if text_tokens_with_lemma == text_tokens:
        print("âœ… æ‰€æœ‰textç±»å‹tokenéƒ½æœ‰lemma")
    else:
        print(f"âŒ æœ‰ {text_tokens - text_tokens_with_lemma} ä¸ªtextç±»å‹tokenæ²¡æœ‰lemma")
    
    # éªŒè¯2: étextç±»å‹çš„tokenæ²¡æœ‰lemma
    non_text_tokens = total_tokens - text_tokens
    if null_lemma_tokens >= non_text_tokens:
        print("âœ… æ‰€æœ‰étextç±»å‹tokenéƒ½æ²¡æœ‰lemma")
    else:
        print(f"âŒ æœ‰ {non_text_tokens - null_lemma_tokens} ä¸ªétextç±»å‹tokenæœ‰lemma")
    
    # æ˜¾ç¤ºä¸€äº›lemmaç¤ºä¾‹
    print(f"\nğŸ“‹ Lemmaç¤ºä¾‹:")
    print("-" * 30)
    
    lemma_examples = []
    for sentence in original_text.text_by_sentence:
        for token in sentence.tokens:
            if token.token_type == "text" and token.lemma:
                lemma_examples.append((token.token_body, token.lemma))
                if len(lemma_examples) >= 5:
                    break
        if len(lemma_examples) >= 5:
            break
    
    for i, (token_body, lemma) in enumerate(lemma_examples, 1):
        print(f"   {i}. '{token_body}' -> '{lemma}'")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_lemma_integration() 