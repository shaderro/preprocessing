#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•token_to_vocabæ¨¡å—
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.token_data import Token, Sentence, OriginalText
from src.utils.token_to_vocab import TokenToVocabConverter, convert_token_to_vocab

def test_token_to_vocab():
    """æµ‹è¯•tokenåˆ°vocabè½¬æ¢åŠŸèƒ½"""
    
    print("ğŸ” æµ‹è¯•token_to_vocabæ¨¡å—")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_tokens = [
        Token(
            token_body="Artificial",
            token_type="text",
            difficulty_level="hard",
            global_token_id=1,
            sentence_token_id=1,
            pos_tag="ADJ",
            lemma="artificial",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="intelligence",
            token_type="text",
            difficulty_level="hard",
            global_token_id=2,
            sentence_token_id=2,
            pos_tag="NOUN",
            lemma="intelligence",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="the",
            token_type="text",
            difficulty_level="easy",
            global_token_id=3,
            sentence_token_id=3,
            pos_tag="DET",
            lemma="the",
            is_grammar_marker=False,
            linked_vocab_id=None
        )
    ]
    
    # åˆ›å»ºæµ‹è¯•å¥å­
    test_sentence = Sentence(
        text_id=1,
        sentence_id=1,
        sentence_body="Artificial intelligence has revolutionized technology.",
        grammar_annotations=[],
        vocab_annotations=[],
        tokens=test_tokens
    )
    
    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬
    test_text = OriginalText(
        text_id=1,
        text_title="æµ‹è¯•æ–‡æœ¬",
        text_by_sentence=[test_sentence]
    )
    
    print(f"ğŸ“ æµ‹è¯•å¥å­: {test_sentence.sentence_body}")
    print(f"ğŸ”¤ æµ‹è¯•tokens: {[token.token_body for token in test_tokens]}")
    print("-" * 30)
    
    try:
        # æµ‹è¯•å•ä¸ªtokenè½¬æ¢
        print("1. æµ‹è¯•å•ä¸ªtokenè½¬æ¢:")
        for token in test_tokens:
            if token.difficulty_level == "hard":
                vocab = convert_token_to_vocab(token, test_sentence.sentence_body, 1, 1)
                if vocab:
                    print(f"   âœ… '{token.token_body}' -> vocab_id: {vocab.vocab_id}")
                else:
                    print(f"   âŒ '{token.token_body}' -> è½¬æ¢å¤±è´¥")
            else:
                print(f"   âšª '{token.token_body}' -> è·³è¿‡ï¼ˆéhardéš¾åº¦ï¼‰")
        
        print()
        
        # æµ‹è¯•æ‰¹é‡è½¬æ¢
        print("2. æµ‹è¯•æ‰¹é‡è½¬æ¢:")
        converter = TokenToVocabConverter("test_vocab_data.json")
        vocab_expressions = converter.convert_tokens_from_text(test_text)
        
        print(f"   æ‰¾åˆ° {len(vocab_expressions)} ä¸ªhardéš¾åº¦çš„vocab:")
        for vocab in vocab_expressions:
            print(f"   - {vocab.vocab_body}: {vocab.explanation[:50]}...")
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        if vocab_expressions:
            converter.save_vocab_data(vocab_expressions)
        
        print()
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_token_to_vocab() 