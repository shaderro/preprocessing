#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.text_processor import TextProcessor

def test_difficulty_integration():
    """æµ‹è¯•éš¾åº¦è¯„ä¼°é›†æˆåŠŸèƒ½"""
    
    print("ğŸ” æµ‹è¯•éš¾åº¦è¯„ä¼°é›†æˆåŠŸèƒ½")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = "Artificial intelligence has revolutionized the way we interact with technology."
    
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
    print("-" * 30)
    
    # å¤„ç†æ–‡æœ¬
    original_text = processor.process_text_to_structured_data(test_text, 999, "æµ‹è¯•æ–‡æœ¬")
    
    # åˆ†æç»“æœ
    print("ğŸ“Š åˆ†æç»“æœ:")
    print("-" * 30)
    
    total_tokens = 0
    text_tokens = 0
    easy_tokens = 0
    hard_tokens = 0
    null_tokens = 0
    
    for sentence in original_text.text_by_sentence:
        print(f"\nğŸ“„ å¥å­ {sentence.sentence_id}: {sentence.sentence_body}")
        print("   Tokens:")
        
        for token in sentence.tokens:
            total_tokens += 1
            difficulty = token.difficulty_level
            
            if token.token_type == "text":
                text_tokens += 1
                if difficulty == "easy":
                    easy_tokens += 1
                    difficulty_mark = "ğŸŸ¢"
                elif difficulty == "hard":
                    hard_tokens += 1
                    difficulty_mark = "ğŸ”´"
                else:
                    null_tokens += 1
                    difficulty_mark = "âšª"
            else:
                difficulty_mark = "âšª"
                if difficulty is None:
                    null_tokens += 1
                else:
                    print(f"âš ï¸  è­¦å‘Šï¼šétextç±»å‹token '{token.token_body}' æœ‰éš¾åº¦çº§åˆ«: {difficulty}")
            
            print(f"     {difficulty_mark} '{token.token_body}' ({token.token_type}) -> {difficulty}")
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print("-" * 30)
    print(f"   æ€»tokenæ•°é‡: {total_tokens}")
    print(f"   textç±»å‹token: {text_tokens}")
    print(f"   easyéš¾åº¦: {easy_tokens}")
    print(f"   hardéš¾åº¦: {hard_tokens}")
    print(f"   nulléš¾åº¦: {null_tokens}")
    
    # éªŒè¯è§„åˆ™
    print(f"\nâœ… éªŒè¯ç»“æœ:")
    print("-" * 30)
    
    # éªŒè¯1: åªæœ‰textç±»å‹çš„tokenæœ‰éš¾åº¦è¯„ä¼°
    text_tokens_with_difficulty = easy_tokens + hard_tokens
    if text_tokens_with_difficulty == text_tokens:
        print("âœ… æ‰€æœ‰textç±»å‹tokenéƒ½æœ‰éš¾åº¦è¯„ä¼°")
    else:
        print(f"âŒ æœ‰ {text_tokens - text_tokens_with_difficulty} ä¸ªtextç±»å‹tokenæ²¡æœ‰éš¾åº¦è¯„ä¼°")
    
    # éªŒè¯2: étextç±»å‹çš„tokenæ²¡æœ‰éš¾åº¦è¯„ä¼°
    non_text_tokens = total_tokens - text_tokens
    if null_tokens >= non_text_tokens:
        print("âœ… æ‰€æœ‰étextç±»å‹tokenéƒ½æ²¡æœ‰éš¾åº¦è¯„ä¼°")
    else:
        print(f"âŒ æœ‰ {non_text_tokens - null_tokens} ä¸ªétextç±»å‹tokenæœ‰éš¾åº¦è¯„ä¼°")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_difficulty_integration() 